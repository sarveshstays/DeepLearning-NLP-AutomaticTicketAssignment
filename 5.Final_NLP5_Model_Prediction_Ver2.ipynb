{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone_Interim_NLP5_Model_Prediction_Ver1.2_lemma.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AZozN0HMdCR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZSi3GqLeulx",
        "colab_type": "text"
      },
      "source": [
        "This file will help predict the incoming ticket data as to which group it will fall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t50JRvhKJ0Zk",
        "colab_type": "code",
        "outputId": "1a9fbaa0-e51e-47b4-e130-ad2a1e5e00c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import all necessary libraries\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import string\n",
        "import math\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score, roc_curve, auc\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from collections import OrderedDict\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Input,Flatten\n",
        "from keras.initializers import Constant\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltpo_jFsK90X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set your project path \n",
        "project_path =  '/content/drive/My Drive/Colab Notebooks/DataSets'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GflekmCULAvY",
        "colab_type": "code",
        "outputId": "ea53b086-0093-494c-a0ed-04f8a0f4bb90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-MOBJBTLFdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the google drive directory path to read and save all files there\n",
        "os.chdir(project_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpG98rEkJ7OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taking groups which have ticket counts greater than 200 in one part and remaining in the other part\n",
        "\n",
        "#L12 = ['GRP_0', 'GRP_8', 'GRP_24', 'GRP_12', 'GRP_9', 'GRP_2', 'GRP_19', 'GRP_3']\n",
        "# L12 = ['GRP_0', 'GRP_8','GRP_9']\n",
        "#L3 = [ 'GRP_1', 'GRP_4', 'GRP_5', 'GRP_6', 'GRP_7', 'GRP_10', 'GRP_11',  'GRP_13', 'GRP_14', 'GRP_15', 'GRP_16', 'GRP_17', 'GRP_18', \n",
        "#       'GRP_20', 'GRP_21', 'GRP_22', 'GRP_23',  'GRP_25', 'GRP_26', 'GRP_27', 'GRP_28', 'GRP_29', 'GRP_30', 'GRP_31',\n",
        "#       'GRP_33', 'GRP_34', 'GRP_35', 'GRP_36', 'GRP_37', 'GRP_38', 'GRP_39', 'GRP_40', 'GRP_41', 'GRP_42', 'GRP_43', 'GRP_44',\n",
        "#       'GRP_45', 'GRP_46', 'GRP_47', 'GRP_48', 'GRP_49', 'GRP_50',  'GRP_51', 'GRP_52', 'GRP_53', 'GRP_54', 'GRP_55', 'GRP_56',\n",
        "#       'GRP_57', 'GRP_58', 'GRP_59', 'GRP_60', 'GRP_61', 'GRP_32',  'GRP_62', 'GRP_63', 'GRP_64', 'GRP_65',\n",
        "#       'GRP_66', 'GRP_67', 'GRP_68', 'GRP_69', 'GRP_70', 'GRP_71', 'GRP_72', 'GRP_73']\n",
        "L12 = ['GRP_0', 'GRP_8', 'GRP_24', 'GRP_12', 'GRP_9', 'GRP_2', 'GRP_19', 'GRP_3']\n",
        "L3  = ['GRP_10', 'GRP_13', 'GRP_14','GRP_5','GRP_25','GRP_98','GRP_99']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-48EoEufMEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']\n",
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(30000, embedding_dim)\n",
        "   \n",
        "#colsToTrainOn =['CombinedDesc']\n",
        "colsToTrainOn =['LemmaString']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE9011tVKA1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_word_list(text):\n",
        "  ''' Pre process and convert texts to a list of words '''\n",
        "  text = str(text)\n",
        "  text = text.lower().replace('.','').replace(',',' ').replace('-',' ').replace('  ', ' ')\n",
        "  text = text.split()\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY4reSB0ffY8",
        "colab_type": "text"
      },
      "source": [
        "Encode the two groups L12/L3 in 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUP2CQteKFCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SetGrp(text):\n",
        "  ''' this will set the L12 to 0\n",
        "  and L3 to 1'''\n",
        "  \n",
        "  if text in L12:\n",
        "      return 0\n",
        "  if text in L3:\n",
        "      return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reoV0QQTKH5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combinedDesc(row):\n",
        "  ''' to combine the short description and Description column together '''\n",
        "  row['CombinedDesc'] = row['ShortDescSplitted'] + row['DescriptionSplitted']\n",
        "  return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vxubc1lBT5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove punctuations\n",
        "def remove_punctuations(text):\n",
        "  ''' Remove all punctuations from the text column values '''\n",
        "  for punctuation in string.punctuation:\n",
        "      text = text.replace(punctuation, ' ')\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv9UKyziBTtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cant use stop words of nltk, as we have to retain some words which for a meaningful context to the ticket description\n",
        "# so decided our own list of stop words\n",
        "stop_words = []\n",
        "stop_words += [\"good\", \"evening\", \"night\", \"afternoon\", \"ca\", \"nt\", \"i\", \"vip\", \"llv\", \"xyz\", \"cid\", \"image\", \"gmail\",\"co\", \"in\", \"com\", \"ticket\", \"company\", \"received\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_LJtvCHBTqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removeString(data, regex):\n",
        "    return data.str.lower().str.replace(regex.lower(), ' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODTR0qzYBly4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getRegexList():\n",
        "  ''' This function will help to remove all the regex list that is defined below \n",
        "  from the text column for which it will be called '''\n",
        "  \n",
        "  regexList = []\n",
        "  regexList += ['From:(.*)\\r\\n']  # from line\n",
        "  regexList += ['ticket[_]*[\\s]*[0-9]*'] # ticket id\n",
        "  regexList += ['Sent:(.*)\\r\\n']  # sent to line\n",
        "  regexList += ['Received:(.*)\\r\\n']  # received data line\n",
        "  regexList += ['To:(.*)\\r\\n']  # to line\n",
        "  regexList += ['CC:(.*)\\r\\n']  # cc line\n",
        "  regexList += ['\\[cid:(.*)]']  # images cid\n",
        "  regexList += ['https?:[^\\]\\n\\r]+']  # https & http\n",
        "  regexList += ['Subject:']\n",
        "  regexList += ['[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+']  # emails\n",
        "  regexList += ['[0-9][\\-0–90-9 ]+']  # phones\n",
        "  regexList += ['[0-9]']  # numbers\n",
        "  regexList += ['[^a-zA-z 0-9]+']  # anything that is not a letter\n",
        "  regexList += ['[\\r\\n]']  # \\r\\n\n",
        "  regexList += [' [a-zA-Z] ']  # single letters\n",
        "  regexList += [\"  \"]  # double spaces\n",
        "  regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
        "  regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
        "  regexList += ['Subject:']\n",
        "  regexList += ['[^a-zA-Z]']\n",
        "\n",
        "  return regexList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is5_LvA0KKZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readFile():    \n",
        "  ''' This function will read the file that is saved after the EDA completion\n",
        "  Will drop NA values which contitute only 9 rows in total\n",
        "  will apply the normal cleaning methods of removing dupliates, removing some regex list, and punctuations from the text columns\n",
        "  Will convert text to word list\n",
        "  Combine to columns into one\n",
        "  Set Level Groups to 0 or 1 '''\n",
        "\n",
        "\n",
        "#  df = pd.read_excel('TicketAssignments.xlsx')\n",
        "  df = pd.read_excel('TestDataToPredict_Description.xlsx')\n",
        "  df.dropna(subset=['Short description', 'Description'], inplace=True)\n",
        "  \n",
        "  df['Short description'] = (df['Short description'].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))\n",
        "  df['Short description'] = df['Short description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "  df['Short description'] = df['Short description'].apply(remove_punctuations)\n",
        "  df['Description'] = (df['Description'].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))\n",
        "  df['Description'] = df['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "  df['Description'] = df['Description'].apply(remove_punctuations)\n",
        "\n",
        "  for regex in getRegexList():\n",
        "    df['Description'] = removeString(df['Description'], regex)\n",
        "    \n",
        "  df['ShortDescSplitted'] = df['Short description'].apply(lambda x : text_to_word_list(x))\n",
        "  df['DescriptionSplitted'] = df['Description'].apply(lambda x : text_to_word_list(x))\n",
        "  \n",
        "  # first assign unknown and then will fill the data\n",
        "  df['CombinedDesc'] = 'UNK9999'\n",
        "  df = df.apply(lambda x :combinedDesc(x), axis=1 )\n",
        "  df['CombinedDescRetained'] = df['CombinedDesc']\n",
        "  df['Level1Grp'] = 0\n",
        "  df['Level1Grp'] = df['Assignment group'].apply(lambda x : SetGrp(x))\n",
        "  df['Assignmentgroup'] = df['Assignment group']\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPfvR6ZcKOuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepDataForPrediction(df,vocabulary,inverse_vocabulary):\n",
        "  ''' Will create the vocabulary and inverse_vocabulary list to be used in embedding \n",
        "  and later to be referred when we will call the same in our predict section '''\n",
        "  \n",
        "  for dataset in [df]:\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "      # Iterate through the text of description column of the row\n",
        "      for question in colsToTrainOn:\n",
        "        q2n = []  \n",
        "        for word in row[question]:\n",
        "          if word not in vocabulary:\n",
        "              vocabulary[word] = len(inverse_vocabulary)\n",
        "              q2n.append(len(inverse_vocabulary))\n",
        "              inverse_vocabulary.append(word)\n",
        "          else:\n",
        "              q2n.append(vocabulary[word])\n",
        "            # Replace description as word to description as number representation\n",
        "          dataset.at[index, question]= q2n\n",
        "  return df, vocabulary,inverse_vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-x_M-xEKjU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepDataForModel(df, maxlen):\n",
        "  X = df[colsToTrainOn[0]]\n",
        "  X = pad_sequences(X, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJLotwfoismn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepDataForModel2_Grp0(df,max_seq_length):\n",
        "    \n",
        "  partDfZero = df.loc[df['Level1Grp'] == 0]\n",
        "  partDfOne = df.loc[df['Level1Grp'] == 1]\n",
        "\n",
        "  le = LabelEncoder()\n",
        "\n",
        "  partDfZero['Assignmentgroup'] = le.fit_transform(partDfZero['Assignment group'])\n",
        "  partDfOne['Assignmentgroup'] = le.fit_transform(partDfOne['Assignment group'])\n",
        "\n",
        "\n",
        "  X_Zero = partDfZero[colsToTrainOn[0]]\n",
        "  Y_Zero = np_utils.to_categorical(partDfZero['Assignmentgroup'])\n",
        "\n",
        "  X_One = partDfOne[colsToTrainOn[0]]\n",
        "  Y_One = np_utils.to_categorical(partDfOne['Assignmentgroup'])\n",
        "      \n",
        "  X_train_Zero, X_validation_Zero, Y_train_Zero, Y_validation_Zero = train_test_split(X_Zero, Y_Zero, test_size=math.floor(len(X_Zero)*0.2))\n",
        "  X_train_One, X_validation_One, Y_train_One, Y_validation_One = train_test_split(X_One, Y_One, test_size=math.floor(len(X_One)*0.2))\n",
        "\n",
        "  X_train_Zero = pad_sequences(X_train_Zero, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "  X_validation_Zero = pad_sequences(X_validation_Zero, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "\n",
        "  X_train_One = pad_sequences(X_train_One, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "  X_validation_One = pad_sequences(X_validation_One, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "\n",
        "\n",
        "  return X_train_Zero, Y_train_Zero, X_validation_Zero,Y_validation_Zero,X_train_One,X_validation_One,Y_train_One,Y_validation_One,partDfZero,partDfOne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3g4ZnP8jixj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepDataForModel_2(df, maxlen):\n",
        "  partDfZero = df.loc[df['PredictedClassLabel'] == 0]\n",
        "  partDfOne = df.loc[df['PredictedClassLabel'] == 1]\n",
        "\n",
        "  X_Zero = partDfZero[colsToTrainOn[0]]\n",
        "  X_Zero = pad_sequences(X_Zero, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "\n",
        "\n",
        "  X_One = partDfOne[colsToTrainOn[0]]\n",
        "  X_One = pad_sequences(X_One, maxlen=maxlen,truncating='post',padding='post',value=0)\n",
        "\n",
        "  return X_Zero, X_One,partDfZero,partDfOne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOpVl9qhKvfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BuildEmbeddingMatrix(word2vec,embeddings):\n",
        "  count = 0    \n",
        "  for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "      count+=1\n",
        "      embeddings[index] = word2vec.word_vec(word)\n",
        "    else:\n",
        "      embeddings[index] = 0\n",
        "\n",
        "  return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDK-UPvKMC-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReadPickeledObjects():\n",
        "  with open('word2vec.pickle', 'rb') as f:\n",
        "    word2vec = pickle.load(f)\n",
        "      \n",
        "  with open('embeddings.pickle', 'rb') as f:\n",
        "    embeddings = pickle.load(f)\n",
        "      \n",
        "  with open('vocabulary.pickle', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)\n",
        "      \n",
        "  with open('inverse_vocabulary.pickle', 'rb') as f:\n",
        "    inverse_vocabulary = pickle.load(f)\n",
        "      \n",
        "  return word2vec,embeddings,vocabulary,inverse_vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2ODCUlfG4sW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9f28db30-a82a-4d28-816d-36a36e53f192"
      },
      "source": [
        "df = pd.read_excel('TestDataToPredict.xlsx')\n",
        "df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LemmaString</th>\n",
              "      <th>NewAssignmentGroup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>outlook team meeting skype not appear calendar...</td>\n",
              "      <td>GRP_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user want reset password</td>\n",
              "      <td>GRP_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>engineer tool not connect unable submit report</td>\n",
              "      <td>GRP_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>duplication network address gentles device sha...</td>\n",
              "      <td>GRP_98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>job fail scheduler monitor tool</td>\n",
              "      <td>GRP_8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         LemmaString NewAssignmentGroup\n",
              "0  outlook team meeting skype not appear calendar...              GRP_0\n",
              "1                           user want reset password              GRP_0\n",
              "2     engineer tool not connect unable submit report              GRP_0\n",
              "3  duplication network address gentles device sha...             GRP_98\n",
              "4                    job fail scheduler monitor tool              GRP_8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGmjoBVI0F7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0b7ec8fa-2ac3-4880-b934-b13014d9f592"
      },
      "source": [
        "  df['Level1Grp'] = 0\n",
        "  df['Level1Grp'] = df['NewAssignmentGroup'].apply(lambda x : SetGrp(x))\n",
        "  #df['Assignmentgroup'] = df['NewAssignmentGroup']\n",
        "  df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LemmaString</th>\n",
              "      <th>NewAssignmentGroup</th>\n",
              "      <th>Level1Grp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>outlook team meeting skype not appear calendar...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user want reset password</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>engineer tool not connect unable submit report</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>duplication network address gentles device sha...</td>\n",
              "      <td>GRP_98</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>job fail scheduler monitor tool</td>\n",
              "      <td>GRP_8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         LemmaString  ... Level1Grp\n",
              "0  outlook team meeting skype not appear calendar...  ...         0\n",
              "1                           user want reset password  ...         0\n",
              "2     engineer tool not connect unable submit report  ...         0\n",
              "3  duplication network address gentles device sha...  ...         1\n",
              "4                    job fail scheduler monitor tool  ...         0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQxFQ6rlOJJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "482a5b97-3487-4f16-e58e-83d0f4ab0105"
      },
      "source": [
        "#df = readFile()\n",
        "#maxlen = min(df['CombinedDesc'].map(lambda x:len(x)).max(), 150)\n",
        "maxlen = min(df['LemmaString'].map(lambda x:len(x)).max(), 150)\n",
        "print(maxlen)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_w6GOqMKDd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec,embeddings,vocabulary,inverse_vocabulary = ReadPickeledObjects() \n",
        "\n",
        "df, vocabulary,inverse_vocabulary= PrepDataForPrediction(df,vocabulary,inverse_vocabulary)\n",
        "  \n",
        "embeddings = BuildEmbeddingMatrix(word2vec,embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWBOyOaNH61A",
        "colab_type": "code",
        "outputId": "bb5d14bc-5516-4e6d-9c82-c1c53558d27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#df['CombinedDesc'].map(lambda x:len(x)).max()\n",
        "df['LemmaString'].map(lambda x:len(x)).max()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfV1_pA9JQyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxlen=150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbt0b8LioyCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlQg3ct-jq5k",
        "colab_type": "code",
        "outputId": "6d5314c7-97a1-4a35-f507-b4920ff6db1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "mode_FirstLayer= tf.keras.models.load_model('model_FirstLevelGrouping.h5')\n",
        "mode_SecondLayerZeroGroup= tf.keras.models.load_model('model_L12_AssignmentGroups.h5')\n",
        "mode_SecondLayerFirstGroup= tf.keras.models.load_model('model_L3_AssignmentGroups.h5')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJ3iOzDjrqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First Model Preidction\n",
        "X = PrepDataForModel(df, maxlen)\n",
        "y = mode_FirstLayer.predict(X)\n",
        "predictedClass = np.argmax(y,axis=1).tolist()\n",
        "df['PredictedClassLabel'] = pd.Series(predictedClass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuKG5k8uUFBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a1da4d97-27fc-4326-d4d4-75ddc86f3dcf"
      },
      "source": [
        "print(X.shape)\n",
        "X"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 150)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  8, 15, ...,  0,  0,  0],\n",
              "       [ 8,  7,  9, ...,  0,  0,  0],\n",
              "       [ 9,  5,  3, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 1, 16, 23, ...,  0,  0,  0],\n",
              "       [ 5,  9,  9, ...,  0,  0,  0],\n",
              "       [ 9, 11, 18, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Az18eGFTVd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c45c0440-f828-4cef-80f4-1bfbe2e7079e"
      },
      "source": [
        "print(y.shape)\n",
        "y"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9491113e-01, 4.4445507e-03],\n",
              "       [9.4871271e-01, 4.3404773e-02],\n",
              "       [9.9516368e-01, 4.3767062e-03],\n",
              "       [9.9370390e-01, 6.9068917e-03],\n",
              "       [9.9964881e-01, 4.0190705e-04],\n",
              "       [4.6511171e-03, 9.9473971e-01],\n",
              "       [7.5164503e-01, 2.3509455e-01],\n",
              "       [4.2601645e-01, 5.2969080e-01],\n",
              "       [4.1483190e-02, 9.5925528e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMXPO66bCS3C",
        "colab_type": "code",
        "outputId": "e65c1b4c-d42b-43c8-91ef-e7282e992baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LemmaString</th>\n",
              "      <th>NewAssignmentGroup</th>\n",
              "      <th>Level1Grp</th>\n",
              "      <th>PredictedClassLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[2, 8, 15, 1, 2, 2, 21, 6, 15, 9, 16, 17, 6, 1...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[8, 7, 9, 11, 6, 22, 16, 5, 15, 6, 11, 9, 7, 9...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[9, 5, 3, 4, 5, 9, 9, 11, 6, 15, 2, 2, 1, 6, 5...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[14, 8, 18, 1, 4, 19, 16, 15, 4, 2, 5, 6, 5, 9...</td>\n",
              "      <td>GRP_98</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[26, 2, 23, 6, 12, 16, 4, 1, 6, 7, 19, 20, 9, ...</td>\n",
              "      <td>GRP_8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         LemmaString  ... PredictedClassLabel\n",
              "0  [2, 8, 15, 1, 2, 2, 21, 6, 15, 9, 16, 17, 6, 1...  ...                   0\n",
              "1  [8, 7, 9, 11, 6, 22, 16, 5, 15, 6, 11, 9, 7, 9...  ...                   0\n",
              "2  [9, 5, 3, 4, 5, 9, 9, 11, 6, 15, 2, 2, 1, 6, 5...  ...                   0\n",
              "3  [14, 8, 18, 1, 4, 19, 16, 15, 4, 2, 5, 6, 5, 9...  ...                   0\n",
              "4  [26, 2, 23, 6, 12, 16, 4, 1, 6, 7, 19, 20, 9, ...  ...                   0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckOP4U9ejrmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Zero, X_One,partDfZero,partDfOne = PrepDataForModel_2(df, maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYc3d73XSFpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e26bac70-0ac9-416f-d69d-bd17545ff7bc"
      },
      "source": [
        "print(X_Zero.shape)\n",
        "print(X_One.shape)\n",
        "print(partDfZero.shape)\n",
        "print(partDfOne.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 150)\n",
            "(3, 150)\n",
            "(6, 4)\n",
            "(3, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnZL-ajWjrjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = mode_SecondLayerZeroGroup.predict(X_Zero)\n",
        "predictedClassGroupZero = np.argmax(y,axis=1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwhZCtonCflW",
        "colab_type": "code",
        "outputId": "a999ea3d-5a38-412d-8406-79163deacc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "X_One"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11,  9,  7,  9, 15,  6, 18, 16,  7,  7, 22,  2, 11, 14,  6, 20,\n",
              "        14, 15, 20, 13,  6, 20,  7, 15, 14, 14,  6, 18, 16,  7,  7, 22,\n",
              "         2, 11, 14,  6, 17, 16,  5, 16,  3,  9, 17,  9,  5, 15,  6, 15,\n",
              "         2,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0],\n",
              "       [ 5,  9,  9, 14,  6, 22,  4, 11,  9,  1,  9,  7,  7,  6, 17,  2,\n",
              "         8,  7,  9,  6,  1, 16, 18, 15,  2, 18,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0],\n",
              "       [ 9, 11, 18,  6, 15, 11, 16,  4,  5,  6, 11,  2,  2, 17,  6,  5,\n",
              "         2, 15,  6, 19,  2,  5,  5,  9, 19, 15,  6, 23,  9, 16, 17,  9,\n",
              "        11,  6, 11,  9,  2, 18,  9,  5,  6,  8,  7,  9, 11,  6, 19, 16,\n",
              "         1,  1,  6, 11,  9, 12,  9, 11,  6, 15, 21, 15,  6,  4,  5, 18,\n",
              "         1, 16,  5, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyEnt15vjrfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = mode_SecondLayerFirstGroup.predict(X_One)\n",
        "predictedClassGroupOne = np.argmax(y,axis=1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dhI4ZhMChRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predictedClassGroupOne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLyEl3hckNir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['PredictedGroup'] = 'UNK9999'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_Ud85TkNfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count=0\n",
        "for index in partDfZero.index:\n",
        "  x = predictedClassGroupZero[count]\n",
        "  df.at[index,'PredictedGroup'] = L12[x]\n",
        "  count+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdMgr9jZkNcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count=0\n",
        "for index in partDfOne.index:\n",
        "  x = predictedClassGroupOne[count]\n",
        "  df.at[index,'PredictedGroup'] = L3[x]\n",
        "  count+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPkG2HslkTix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_excel('Output.xlsx', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iyK69kQmbGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}